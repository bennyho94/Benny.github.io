{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opdracht SVM & Hyperparametertuning\n",
    "\n",
    "Support Vector Machines behoren nog altijd tot een van de meest populaire en gebruikte ML algoritmes. Ze zijn bijzonder veelzijdig en kunnen ingezet worden bij zowel classificatieproblemen als regressieproblemen en zelfs bij het opsporen van uitschieters.\n",
    "\n",
    "Bij deze opdracht zullen we ons beperken tot SVM's voor classificatie. \n",
    "\n",
    "In een eerste deel hernemen we de cancer detection uit de vorige opdracht (logistic regression) waarbij we nu gebruik maken van een Support Vector Machine classifier.\n",
    "\n",
    "Het tweede deel van de opdracht bestaat erin om handschriftherkenner te bouwen. We gebruiken daarvoor de MNIST dataset die bestaat uit duizenden afbeeldingen van handgeschreven cijfers.                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#from skopt import BayesSearchCV\n",
    "from scipy.stats import randint \n",
    "from scipy.stats import uniform\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.io import imread, imshow\n",
    "\n",
    "# scaling\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# lineare model\n",
    "from sklearn import linear_model\n",
    "\n",
    "# polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# svm\n",
    "from sklearn import svm\n",
    "\n",
    "# poly \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opdracht 1 - Cancer detection via SVM\n",
    "\n",
    "Doe de cancer detection uit vorige opdracht opnieuw maar nu gebruik makende van Support Vector Machines. \n",
    "Beantwoord ook telkens de vragen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancer detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>843786</td>\n",
       "      <td>M</td>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>...</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>86211</td>\n",
       "      <td>B</td>\n",
       "      <td>12.180</td>\n",
       "      <td>17.84</td>\n",
       "      <td>77.79</td>\n",
       "      <td>451.1</td>\n",
       "      <td>0.10450</td>\n",
       "      <td>0.07057</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.029410</td>\n",
       "      <td>...</td>\n",
       "      <td>20.92</td>\n",
       "      <td>82.14</td>\n",
       "      <td>495.2</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.09358</td>\n",
       "      <td>0.04980</td>\n",
       "      <td>0.05882</td>\n",
       "      <td>0.2227</td>\n",
       "      <td>0.07376</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>862261</td>\n",
       "      <td>B</td>\n",
       "      <td>9.787</td>\n",
       "      <td>19.94</td>\n",
       "      <td>62.11</td>\n",
       "      <td>294.5</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.05301</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>...</td>\n",
       "      <td>26.29</td>\n",
       "      <td>68.81</td>\n",
       "      <td>366.1</td>\n",
       "      <td>0.1316</td>\n",
       "      <td>0.09473</td>\n",
       "      <td>0.02049</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.1934</td>\n",
       "      <td>0.08988</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>862485</td>\n",
       "      <td>B</td>\n",
       "      <td>11.600</td>\n",
       "      <td>12.84</td>\n",
       "      <td>74.34</td>\n",
       "      <td>412.6</td>\n",
       "      <td>0.08983</td>\n",
       "      <td>0.07525</td>\n",
       "      <td>0.041960</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>...</td>\n",
       "      <td>17.16</td>\n",
       "      <td>82.96</td>\n",
       "      <td>512.5</td>\n",
       "      <td>0.1431</td>\n",
       "      <td>0.18510</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.08449</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>0.08756</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>862548</td>\n",
       "      <td>M</td>\n",
       "      <td>14.420</td>\n",
       "      <td>19.77</td>\n",
       "      <td>94.48</td>\n",
       "      <td>642.5</td>\n",
       "      <td>0.09752</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.093880</td>\n",
       "      <td>0.058390</td>\n",
       "      <td>...</td>\n",
       "      <td>30.86</td>\n",
       "      <td>109.50</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.1431</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.31940</td>\n",
       "      <td>0.15650</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>862717</td>\n",
       "      <td>M</td>\n",
       "      <td>13.610</td>\n",
       "      <td>24.98</td>\n",
       "      <td>88.05</td>\n",
       "      <td>582.7</td>\n",
       "      <td>0.09488</td>\n",
       "      <td>0.08511</td>\n",
       "      <td>0.086250</td>\n",
       "      <td>0.044890</td>\n",
       "      <td>...</td>\n",
       "      <td>35.27</td>\n",
       "      <td>108.60</td>\n",
       "      <td>906.5</td>\n",
       "      <td>0.1265</td>\n",
       "      <td>0.19430</td>\n",
       "      <td>0.31690</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.2651</td>\n",
       "      <td>0.07397</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0     842517         M       20.570         17.77          132.90     1326.0   \n",
       "1   84300903         M       19.690         21.25          130.00     1203.0   \n",
       "2   84348301         M       11.420         20.38           77.58      386.1   \n",
       "3   84358402         M       20.290         14.34          135.10     1297.0   \n",
       "4     843786         M       12.450         15.70           82.57      477.1   \n",
       "..       ...       ...          ...           ...             ...        ...   \n",
       "95     86211         B       12.180         17.84           77.79      451.1   \n",
       "96    862261         B        9.787         19.94           62.11      294.5   \n",
       "97    862485         B       11.600         12.84           74.34      412.6   \n",
       "98    862548         M       14.420         19.77           94.48      642.5   \n",
       "99    862717         M       13.610         24.98           88.05      582.7   \n",
       "\n",
       "    smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0           0.08474           0.07864        0.086900             0.070170   \n",
       "1           0.10960           0.15990        0.197400             0.127900   \n",
       "2           0.14250           0.28390        0.241400             0.105200   \n",
       "3           0.10030           0.13280        0.198000             0.104300   \n",
       "4           0.12780           0.17000        0.157800             0.080890   \n",
       "..              ...               ...             ...                  ...   \n",
       "95          0.10450           0.07057        0.024900             0.029410   \n",
       "96          0.10240           0.05301        0.006829             0.007937   \n",
       "97          0.08983           0.07525        0.041960             0.033500   \n",
       "98          0.09752           0.11410        0.093880             0.058390   \n",
       "99          0.09488           0.08511        0.086250             0.044890   \n",
       "\n",
       "    ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0   ...          23.41           158.80      1956.0            0.1238   \n",
       "1   ...          25.53           152.50      1709.0            0.1444   \n",
       "2   ...          26.50            98.87       567.7            0.2098   \n",
       "3   ...          16.67           152.20      1575.0            0.1374   \n",
       "4   ...          23.75           103.40       741.6            0.1791   \n",
       "..  ...            ...              ...         ...               ...   \n",
       "95  ...          20.92            82.14       495.2            0.1140   \n",
       "96  ...          26.29            68.81       366.1            0.1316   \n",
       "97  ...          17.16            82.96       512.5            0.1431   \n",
       "98  ...          30.86           109.50       826.4            0.1431   \n",
       "99  ...          35.27           108.60       906.5            0.1265   \n",
       "\n",
       "    compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.18660          0.24160               0.18600          0.2750   \n",
       "1             0.42450          0.45040               0.24300          0.3613   \n",
       "2             0.86630          0.68690               0.25750          0.6638   \n",
       "3             0.20500          0.40000               0.16250          0.2364   \n",
       "4             0.52490          0.53550               0.17410          0.3985   \n",
       "..                ...              ...                   ...             ...   \n",
       "95            0.09358          0.04980               0.05882          0.2227   \n",
       "96            0.09473          0.02049               0.02381          0.1934   \n",
       "97            0.18510          0.19220               0.08449          0.2772   \n",
       "98            0.30260          0.31940               0.15650          0.2718   \n",
       "99            0.19430          0.31690               0.11840          0.2651   \n",
       "\n",
       "    fractal_dimension_worst  Unnamed: 32  \n",
       "0                   0.08902          NaN  \n",
       "1                   0.08758          NaN  \n",
       "2                   0.17300          NaN  \n",
       "3                   0.07678          NaN  \n",
       "4                   0.12440          NaN  \n",
       "..                      ...          ...  \n",
       "95                  0.07376          NaN  \n",
       "96                  0.08988          NaN  \n",
       "97                  0.08756          NaN  \n",
       "98                  0.09353          NaN  \n",
       "99                  0.07397          NaN  \n",
       "\n",
       "[100 rows x 33 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inlezen van de dataset\n",
    "\n",
    "dataset = pd.read_csv('cancer.csv')\n",
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                         211\n",
      "diagnosis                  211\n",
      "radius_mean                211\n",
      "texture_mean               211\n",
      "perimeter_mean             211\n",
      "area_mean                  211\n",
      "smoothness_mean            211\n",
      "compactness_mean           211\n",
      "concavity_mean             211\n",
      "concave points_mean        211\n",
      "symmetry_mean              211\n",
      "fractal_dimension_mean     211\n",
      "radius_se                  211\n",
      "texture_se                 211\n",
      "perimeter_se               211\n",
      "area_se                    211\n",
      "smoothness_se              211\n",
      "compactness_se             211\n",
      "concavity_se               211\n",
      "concave points_se          211\n",
      "symmetry_se                211\n",
      "fractal_dimension_se       211\n",
      "radius_worst               211\n",
      "texture_worst              211\n",
      "perimeter_worst            211\n",
      "area_worst                 211\n",
      "smoothness_worst           211\n",
      "compactness_worst          211\n",
      "concavity_worst            211\n",
      "concave points_worst       211\n",
      "symmetry_worst             211\n",
      "fractal_dimension_worst    211\n",
      "Unnamed: 32                  0\n",
      "dtype: int64\n",
      "id                         357\n",
      "diagnosis                  357\n",
      "radius_mean                357\n",
      "texture_mean               357\n",
      "perimeter_mean             357\n",
      "area_mean                  357\n",
      "smoothness_mean            357\n",
      "compactness_mean           357\n",
      "concavity_mean             357\n",
      "concave points_mean        357\n",
      "symmetry_mean              357\n",
      "fractal_dimension_mean     357\n",
      "radius_se                  357\n",
      "texture_se                 357\n",
      "perimeter_se               357\n",
      "area_se                    357\n",
      "smoothness_se              357\n",
      "compactness_se             357\n",
      "concavity_se               357\n",
      "concave points_se          357\n",
      "symmetry_se                357\n",
      "fractal_dimension_se       357\n",
      "radius_worst               357\n",
      "texture_worst              357\n",
      "perimeter_worst            357\n",
      "area_worst                 357\n",
      "smoothness_worst           357\n",
      "compactness_worst          357\n",
      "concavity_worst            357\n",
      "concave points_worst       357\n",
      "symmetry_worst             357\n",
      "fractal_dimension_worst    357\n",
      "Unnamed: 32                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# gebalanceerdheid controleren\n",
    "positive = dataset.loc[dataset['diagnosis'] == 'M'].count()\n",
    "negative = dataset.loc[dataset['diagnosis'] == 'B'].count()\n",
    "print(positive)\n",
    "print(negative)\n",
    "\n",
    "# data is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.680000e+02</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.042382e+07</td>\n",
       "      <td>14.120491</td>\n",
       "      <td>19.305335</td>\n",
       "      <td>91.914754</td>\n",
       "      <td>654.279754</td>\n",
       "      <td>0.096321</td>\n",
       "      <td>0.104036</td>\n",
       "      <td>0.088427</td>\n",
       "      <td>0.048746</td>\n",
       "      <td>0.181055</td>\n",
       "      <td>...</td>\n",
       "      <td>25.691919</td>\n",
       "      <td>107.125053</td>\n",
       "      <td>878.578873</td>\n",
       "      <td>0.132316</td>\n",
       "      <td>0.253541</td>\n",
       "      <td>0.271414</td>\n",
       "      <td>0.114341</td>\n",
       "      <td>0.289776</td>\n",
       "      <td>0.083884</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.251246e+08</td>\n",
       "      <td>3.523416</td>\n",
       "      <td>4.288506</td>\n",
       "      <td>24.285848</td>\n",
       "      <td>351.923751</td>\n",
       "      <td>0.014046</td>\n",
       "      <td>0.052355</td>\n",
       "      <td>0.079294</td>\n",
       "      <td>0.038617</td>\n",
       "      <td>0.027319</td>\n",
       "      <td>...</td>\n",
       "      <td>6.141662</td>\n",
       "      <td>33.474687</td>\n",
       "      <td>567.846267</td>\n",
       "      <td>0.022818</td>\n",
       "      <td>0.156523</td>\n",
       "      <td>0.207989</td>\n",
       "      <td>0.065484</td>\n",
       "      <td>0.061508</td>\n",
       "      <td>0.018017</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692225e+05</td>\n",
       "      <td>11.697500</td>\n",
       "      <td>16.177500</td>\n",
       "      <td>75.135000</td>\n",
       "      <td>420.175000</td>\n",
       "      <td>0.086290</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0.029540</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>...</td>\n",
       "      <td>21.095000</td>\n",
       "      <td>84.102500</td>\n",
       "      <td>514.975000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.146900</td>\n",
       "      <td>0.114475</td>\n",
       "      <td>0.064730</td>\n",
       "      <td>0.250350</td>\n",
       "      <td>0.071412</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.061570e+05</td>\n",
       "      <td>13.355000</td>\n",
       "      <td>18.855000</td>\n",
       "      <td>86.210000</td>\n",
       "      <td>548.750000</td>\n",
       "      <td>0.095865</td>\n",
       "      <td>0.092525</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.033455</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.425000</td>\n",
       "      <td>97.655000</td>\n",
       "      <td>685.550000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211850</td>\n",
       "      <td>0.226550</td>\n",
       "      <td>0.099840</td>\n",
       "      <td>0.282050</td>\n",
       "      <td>0.080015</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.825022e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.802500</td>\n",
       "      <td>103.875000</td>\n",
       "      <td>782.625000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.129650</td>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.195625</td>\n",
       "      <td>...</td>\n",
       "      <td>29.757500</td>\n",
       "      <td>125.175000</td>\n",
       "      <td>1073.500000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.337600</td>\n",
       "      <td>0.381400</td>\n",
       "      <td>0.161325</td>\n",
       "      <td>0.317675</td>\n",
       "      <td>0.092065</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count  5.680000e+02   568.000000    568.000000      568.000000   568.000000   \n",
       "mean   3.042382e+07    14.120491     19.305335       91.914754   654.279754   \n",
       "std    1.251246e+08     3.523416      4.288506       24.285848   351.923751   \n",
       "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
       "25%    8.692225e+05    11.697500     16.177500       75.135000   420.175000   \n",
       "50%    9.061570e+05    13.355000     18.855000       86.210000   548.750000   \n",
       "75%    8.825022e+06    15.780000     21.802500      103.875000   782.625000   \n",
       "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       568.000000        568.000000      568.000000           568.000000   \n",
       "mean          0.096321          0.104036        0.088427             0.048746   \n",
       "std           0.014046          0.052355        0.079294             0.038617   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086290          0.064815        0.029540             0.020310   \n",
       "50%           0.095865          0.092525        0.061400             0.033455   \n",
       "75%           0.105300          0.130400        0.129650             0.073730   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  ...  texture_worst  perimeter_worst   area_worst  \\\n",
       "count     568.000000  ...     568.000000       568.000000   568.000000   \n",
       "mean        0.181055  ...      25.691919       107.125053   878.578873   \n",
       "std         0.027319  ...       6.141662        33.474687   567.846267   \n",
       "min         0.106000  ...      12.020000        50.410000   185.200000   \n",
       "25%         0.161900  ...      21.095000        84.102500   514.975000   \n",
       "50%         0.179200  ...      25.425000        97.655000   685.550000   \n",
       "75%         0.195625  ...      29.757500       125.175000  1073.500000   \n",
       "max         0.304000  ...      49.540000       251.200000  4254.000000   \n",
       "\n",
       "       smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "count        568.000000         568.000000       568.000000   \n",
       "mean           0.132316           0.253541         0.271414   \n",
       "std            0.022818           0.156523         0.207989   \n",
       "min            0.071170           0.027290         0.000000   \n",
       "25%            0.116600           0.146900         0.114475   \n",
       "50%            0.131300           0.211850         0.226550   \n",
       "75%            0.146000           0.337600         0.381400   \n",
       "max            0.222600           1.058000         1.252000   \n",
       "\n",
       "       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
       "count            568.000000      568.000000               568.000000   \n",
       "mean               0.114341        0.289776                 0.083884   \n",
       "std                0.065484        0.061508                 0.018017   \n",
       "min                0.000000        0.156500                 0.055040   \n",
       "25%                0.064730        0.250350                 0.071412   \n",
       "50%                0.099840        0.282050                 0.080015   \n",
       "75%                0.161325        0.317675                 0.092065   \n",
       "max                0.291000        0.663800                 0.207500   \n",
       "\n",
       "       Unnamed: 32  \n",
       "count          0.0  \n",
       "mean           NaN  \n",
       "std            NaN  \n",
       "min            NaN  \n",
       "25%            NaN  \n",
       "50%            NaN  \n",
       "75%            NaN  \n",
       "max            NaN  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568, 30)\n",
      "Shape: (397, 30)\n",
      "(171, 30)\n"
     ]
    }
   ],
   "source": [
    "# Verwijder de laatste kolom en de kolom id\n",
    "\n",
    "dataset.drop('id', axis=1, inplace=True)\n",
    "dataset.drop('Unnamed: 32', axis=1, inplace=True)\n",
    "\n",
    "# Vervangen van B en M door 0 en 1 in de output diagnosis\n",
    "dataset['diagnosis'] = dataset['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Opsplitsen in features en targets\n",
    "X = dataset.drop(['diagnosis'], axis=1).values\n",
    "y = dataset['diagnosis'].values\n",
    "print(X.shape)\n",
    "# Opsplitsen in training set en test set. Zorg dat de test set 1/3 is van de totale dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print('Shape:',X_train.shape)\n",
    "print(X_test.shape)\n",
    "# Normalisatie van de features\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train een logistic regression model via cross-validation.\n",
    "\n",
    "- Gebruik daarvoor zowel grid search, random search en Bayes optimization om op zoek te gaan naar de beste hyperparameters: C-waarde, class_weight, solver. Meer info: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Varieer bij K-fold cross-validation de waarde van K. Bespreek de resultaten.\n",
    "- Test de bekomen modellen op de test set. Welke search techniek geniet jouw voorkeur en waarom?\n",
    "- Heeft het zin om de featureset uit te breiden met polynomial features? Test dit. http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialiseren en trainen van de logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainen van logistic regression via cross-validation\n",
    "\n",
    "# Initialiseren van de logistic regression classifier\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1, class_weight = None, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De bias = theta_0:  [-0.02826018]\n",
      "De coefficienten theta:  [[ 0.23586972  0.08268292  0.25348959  0.36273746  0.37009201 -0.24460733\n",
      "   0.7156858   0.63232973 -0.0659836  -0.30363277  1.13561566 -0.21658861\n",
      "   0.59205099  0.93352236  0.32743536 -0.79838143 -0.08201114  0.06108378\n",
      "  -0.4376749  -0.44216356  1.04667627  1.20969075  0.89945153  1.070317\n",
      "   0.43984698  0.12875646  0.71563285  0.94709849  0.89842922  0.42192042]]\n"
     ]
    }
   ],
   "source": [
    "# Training van het model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('De bias = theta_0: ', logreg.intercept_)\n",
    "print('De coefficienten theta: ',logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen van het model op de test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       112\n",
      "           1       0.98      0.95      0.97        59\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.97      0.97       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n",
      "[[111   1]\n",
      " [  3  56]]\n",
      "97.6608187134503\n"
     ]
    }
   ],
   "source": [
    "# Testen van het model op de test set\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalueren van het model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.47 %\n",
      "Standard Deviation: 2.08 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = logreg, X = X_train, y = y_train, cv = 12)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultaten:\n",
    "- cv = 5:\n",
    "    - Accuracy: 96.72 %\n",
    "    - Standard Deviation: 1.72%\n",
    "- cv = 10:\n",
    "    - Accuracy: 96.99 %\n",
    "    - Standard Deviation: 2.69%\n",
    "- cv = 12\n",
    "    - Accuracy: 97.47 %\n",
    "    - Standard Deviation: 2.08%\n",
    "- cv = 15:\n",
    "    - Accuracy: 97.23 %\n",
    "    - Standard Deviation: 2.93%\n",
    "- cv = 20:\n",
    "    - Accuracy: 96.99%\n",
    "    - Standard Deviation: 3.68%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Varieer bij K-fold cross-validation de waarde van K. Bespreek de resultaten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing the K-fold cross-validation I have used several values to see which K value is the most efficient. As you can see above, the value consits of 5, 10, 12, 15, 20. The reason why I opted for a range to 20 is due to the fact that the performance is getting worse once it has passed the 20. Moreover, the value 10 and 20 are similar with an accuracy of 96.99% Therefore, I chose the number 15 as this is the number in the middle. The results are slightly better with an accuracy of 97.47% and a standard deviation of 2.93%. To search for the most efficient k value,  I've tried once more with 12 and the results are better with an accuracy of 97.47% and a standard deviation of 2.08% which means that the range is 95.93% - 99.55% when it comes to accuracy. This is an ideal and reliable cv value when it comes to testing. \n",
    "\n",
    "A small note is that there is a cv value in grid search and random search but I kept it at a minimum to evaluate my model with grid search and random search to reduce the execution time. The higher the cv value, the more reliable is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 40 candidates, totalling 800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 332 tasks      | elapsed:    4.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9798684210526314\n",
      "Best parameters : {'C': 0.25, 'class_weight': None, 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       112\n",
      "           1       0.98      0.93      0.96        59\n",
      "\n",
      "    accuracy                           0.97       171\n",
      "   macro avg       0.97      0.96      0.97       171\n",
      "weighted avg       0.97      0.97      0.97       171\n",
      "\n",
      "[[111   1]\n",
      " [  4  55]]\n",
      "97.07602339181285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:    7.4s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "paramaters = [\n",
    "             {'C' : [0.25, 0.5, 0.75, 1], \n",
    "              'solver': ['liblinear','lbfgs', 'newton-cg', 'sag','saga'], \n",
    "              'class_weight': [None, 'balanced']},\n",
    "             ]\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 20,\n",
    "                           n_jobs = -1,\n",
    "                           verbose=5)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 20 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    1.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9748684210526314\n",
      "Best parameters : {'C': 11.867685804983289, 'class_weight': None, 'solver': 'saga'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       112\n",
      "           1       0.98      0.98      0.98        59\n",
      "\n",
      "    accuracy                           0.99       171\n",
      "   macro avg       0.99      0.99      0.99       171\n",
      "weighted avg       0.99      0.99      0.99       171\n",
      "\n",
      "[[111   1]\n",
      " [  1  58]]\n",
      "98.83040935672514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    3.3s finished\n",
      "C:\\Users\\Dominic Ho\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint \n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "parameters = [\n",
    "             {'C' : uniform(0.01, 20), \n",
    "              'solver': ['liblinear','lbfgs', 'newton-cg', 'sag','saga'],\n",
    "              'class_weight': [None, 'balanced']}\n",
    "             ]\n",
    "\n",
    "              \n",
    "\n",
    "n_iter_search = 20\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=parameters,cv=20,n_iter=n_iter_search,n_jobs = -1,verbose=1)\n",
    "\n",
    "random_search = random_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = random_search.best_score_ \n",
    "best_parameters = random_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', random_search.best_score_)\n",
    "print('Best parameters :',random_search.best_params_  )\n",
    "\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test de bekomen modellen op de test set. Welke search techniek geniet jouw voorkeur en waarom?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "    - Logistic regression on test set:\n",
    "        - Accuracy: 97.66%\n",
    "    - Grid search:\n",
    "        - Best accuracy: 0.9798684210526314 %\n",
    "        - Accuracy: 97.07%\n",
    "    - Random search:\n",
    "        - Best accuracy: 0.9748684210526315 %\n",
    "        - Accuracy: 98.83%\n",
    "\n",
    "The results of the logistic regression amounts 97.66% for the accuracy and the preference goes to the random search method for improving the model as the accuracy amounts 98.83% which has improved slightly.\n",
    "\n",
    "From our confusion matrix, we can see that our model got (111+58) 169 predictions right and (1+1) 2 predictions wrong. We have 1 false positive which means that our model predicted that 1 person is malignant but it turned out not to be. This is actually good. On the other hand, our model also has 1 false negative. What this means is that our model predicted 1 person benign while it is positive. This is bad as the impact for the person will be bigger.\n",
    "\n",
    "Moreover based on the classification report there is a preference for class 0 with 0.99% compared with class 1 with 0.98%. This model is ideal and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Heeft het zin om de featureset uit te breiden met polynomial features? Test dit. http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensie van X_train_poly:  (397, 496)\n",
      "dimensie van X_test_poly:  (171, 496)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       112\n",
      "           1       0.93      0.92      0.92        59\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.94      0.94      0.94       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n",
      "[[108   4]\n",
      " [  5  54]]\n",
      "accuracy op de test set: 94.73684210526315\n",
      "accuracy op de training set: 98.99244332493703\n"
     ]
    }
   ],
   "source": [
    "graad = 2\n",
    "\n",
    "poly = PolynomialFeatures(graad)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "print('dimensie van X_train_poly: ',X_train_poly.shape)\n",
    "print('dimensie van X_test_poly: ',X_test_poly.shape)\n",
    "\n",
    "logreg_poly = linear_model.LogisticRegression(C=0.25, class_weight = None, solver='liblinear')\n",
    "\n",
    "logreg_poly.fit(X_train_poly,y_train)\n",
    "\n",
    "y_pred_poly = logreg_poly.predict(X_test_poly)\n",
    "\n",
    "print(classification_report(y_test,y_pred_poly))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred_poly))\n",
    "\n",
    "print('accuracy op de test set:', accuracy_score(y_test,y_pred_poly)*100)\n",
    "\n",
    "print('accuracy op de training set:', accuracy_score(y_train,logreg_poly.predict(X_train_poly))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see is the accuracy on the test set 94.73% compared with the prefered method with random search. This means that there is no need for using polyonomial features. Moreover the accuracy on the training set is higher and lower on the test set. Also note the higher the degree of the polynomial, the higher the chance of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train een Support Vector Machine via cross-validation\n",
    "\n",
    "- Gebruik daarvoor zowel grid search, random search en Bayes optimization om op zoek te gaan naar de beste hyperparameters: Kernel, C-waarde, Gamma waarde, graad van de polynomial kernel, class_weight. Meer info: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "- Varieer bij K-fold cross-validation de waarde van K. Bespreek de resultaten.\n",
    "- Test de bekomen modellen op de test set. Welke search techniek geniet jouw voorkeur en waarom?\n",
    "- Vergelijk de resultaten met deze die bekomen werden via logistic regression. Welke presteert het best in termen van accuracy, recall, precision en f1 score? Zoek een verklaring. \n",
    "- Merk je een groot verschil in berekeningstijd tussen logistic regression en Support Vector Machines? Verklaar het mogelijks verschil.\n",
    "- Onderzoek of normalisatie van de features een effect heeft op de training tijd (via cross-validation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialiseren en trainen van de SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, kernel='linear')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainen van SVM via cross-validation\n",
    "# zonder kernel\n",
    "SVMlinear = svm.SVC(kernel='linear',C=1)\n",
    "SVMlinear.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen van het model op de test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       112\n",
      "           1       0.98      0.95      0.97        59\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.97      0.97       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n",
      "[[111   1]\n",
      " [  3  56]]\n",
      "97.6608187134503\n"
     ]
    }
   ],
   "source": [
    "y_pred = SVMlinear.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalueren van het model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.47 %\n",
      "Standard Deviation: 2.08 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = SVMlinear, X = X_train, y = y_train, cv = 12)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultaten:\n",
    "- cv = 5:\n",
    "    - Accuracy: 95.46 %\n",
    "    - Standard Deviation: 3.36%\n",
    "- cv = 10:\n",
    "    - Accuracy: 97.24 %\n",
    "    - Standard Deviation: 2.36%\n",
    "- cv = 12\n",
    "    - Accuracy: 97.47 %\n",
    "    - Standard Deviation: 2.08%\n",
    "- cv = 15:\n",
    "    - Accuracy: 96.47 %\n",
    "    - Standard Deviation: 3.53%\n",
    "- cv = 20:\n",
    "    - Accuracy: 96.99%\n",
    "    - Standard Deviation: 4.01%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Varieer bij K-fold cross-validation de waarde van K. Bespreek de resultaten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing the K-fold cross-validation I have used several values to see which K value is the most efficient. As you can see above, the value consists of 5, 10, 12, 15, 20. The reason why I opted for a range to 20 is due to the fact that the performance is getting worse once it has passed the 20. Moreover, the value 10 has an accuracy of 97.24% and 15 an accuracy of 96.47%. To search for the most efficient k value,  I've tried once more with 12 and the results are better with an accuracy of 97.47% and a standard deviation of 2.08% which means that the range is 95.93% - 99.55% when it comes to accuracy. This is an ideal and reliable cv value when it comes to testing. \n",
    "\n",
    "A small note is that there is a cv value in grid search and random search but I kept it at a minimum to evaluate my model with grid search and random search to reduce the execution time. The higher the cv value, the more reliable is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 960 candidates, totalling 19200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 512 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2528 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 5120 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 8288 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done 12032 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done 16352 tasks      | elapsed:   25.1s\n",
      "[Parallel(n_jobs=-1)]: Done 19200 out of 19200 | elapsed:   29.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9823684210526314\n",
      "Best parameters : {'C': 2.231111111111111, 'class_weight': 'balanced', 'degree': 2, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       112\n",
      "           1       0.98      0.90      0.94        59\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.97      0.94      0.95       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n",
      "[[111   1]\n",
      " [  6  53]]\n",
      "95.90643274853801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "paramaters = [ \n",
    "        {'kernel': ['linear'], \n",
    "         'C': np.linspace(0.01,20,10), \n",
    "         'degree': [2,3,4], \n",
    "         'class_weight': [None, 'balanced']},\n",
    "        {'kernel': ['rbf', 'poly', 'sigmoid'], \n",
    "         'C': np.linspace(0.01,20,10), \n",
    "         'gamma': [0.0001, 0.001, 0.01, 0.1, 0.2], \n",
    "         'degree': [2,3,4], \n",
    "         'class_weight': [None, 'balanced']},\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy', \n",
    "                           cv = 20,\n",
    "                           n_jobs = -1,\n",
    "                           verbose =5)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9621518987341771\n",
      "Best parameters : {'C': 0.5901916449666744, 'class_weight': None, 'degree': 2, 'gamma': 0.04173958437179073, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       112\n",
      "           1       1.00      0.90      0.95        59\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.97      0.95      0.96       171\n",
      "weighted avg       0.97      0.96      0.96       171\n",
      "\n",
      "[[112   0]\n",
      " [  6  53]]\n",
      "96.49122807017544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Train een SVM via random search\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint \n",
    "from scipy.stats import uniform\n",
    "\n",
    "\n",
    "#svc trainen\n",
    "model = SVC()\n",
    "\n",
    "parameters = [\n",
    "    {'kernel': ['linear'], \n",
    "     'C': uniform(0.01, 20), \n",
    "     'degree': [2,3,4], \n",
    "     'class_weight': [None, 'balanced']\n",
    "    },\n",
    "    {'kernel': ['rbf', 'poly', 'sigmoid'], \n",
    "     'C': uniform(0.01, 20), \n",
    "     'gamma': uniform(0.001, 0.2), \n",
    "     'degree': [2,3,4], \n",
    "     'class_weight': [None, 'balanced']}\n",
    "]\n",
    " \n",
    "n_iter_search = 20\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=parameters,cv=5,n_iter=n_iter_search,n_jobs = -1,verbose=1)\n",
    "\n",
    "random_search = random_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = random_search.best_score_ \n",
    "best_parameters = random_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', random_search.best_score_)\n",
    "print('Best parameters :',random_search.best_params_  )\n",
    "\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test de bekomen modellen op de test set. Welke search techniek geniet jouw voorkeur en waarom?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "    -SVM on test set:\n",
    "        - Accuracy: 97.66%\n",
    "    - Grid search:\n",
    "        - Best accuracy: 0.9823684210526314 %\n",
    "        - Accuracy : 95.90%\n",
    "    - Random search:\n",
    "        - Best accuracy: 0.9621518987341771 %\n",
    "        - Accuracy : 96.49%\n",
    "\n",
    "The results of SVM amounts 97.66% for the accuracy and the preference goes to the random search method for improving the model as the best accuracy amounts 96.49% which has improved slightly.\n",
    "\n",
    "From our confusion matrix, we can see that our model got (112+53) 165 predictions right and (6+0) 6 predictions wrong.  We have 0 false positive which means that our model predicted that 0 person is malignant but it turned out not to be. This is actually good. On the other hand, our model also has 6 false negative. What this means is that our model predicted 6 people are benign while they are positive. This is bad as the impact for them will be bigger.\n",
    "\n",
    "Moreover based on the classification report there is a preference for class 0 with 0.97% compared with class 1 with 0.95%. This model is ideal and reliable.\n",
    "\n",
    "Conclusion: Based on the results we can say the the logistic regression classifier with random search the best and most efficient model is with an accuracy of 98.83%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vergelijk de resultaten met deze die bekomen werden via logistic regression. Welke presteert het best in termen van accuracy, recall, precision en f1 score? Zoek een verklaring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Logistic regression:\n",
    "    - Grid search:\n",
    "        - Accuracy: 97.07602339181285 %\n",
    "    - Random search:\n",
    "        - Accuracy: 98.83 %\n",
    "\n",
    "SVM:\n",
    "    - Grid search:\n",
    "        - Accuracy: 95.90643274853801 %\n",
    "    - Random search:\n",
    "        - Accuracy: 96.49122807017544 %\n",
    "\n",
    "\n",
    "In general, we can see that the logistic regression classifier with the random search method has the highest scores in terms of accuracy, recall, precision and f1 in the classification report. The accuracy amounts 98%, likewise all the other scores are more than 0.97%. Moreover, the model has a preference in class 0 with 0.98% and class 1 with 0.97% This means that the model is highly reliable and ideal for predicting cancer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merk je een groot verschil in berekeningstijd tussen logistic regression en Support Vector Machines? Verklaar het mogelijks verschil\n",
    "\n",
    "Yes, there is a difference in execution time between logistic regression en support vector machines. On this dataset is the SVM faster than the logistic regression. The reason is due to the fact that SVM are less complex than logistic regression. Logistic regression is also more computable intensive due to the exp function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Onderzoek of normalisatie van de features een effect heeft op de training tijd (via cross-validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, feature scaling with normalisation has an impact on the execution time. If the values of the features are scaled (numbers are smaller) then this means that this is less computational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opdracht 2 - MNIST\n",
    "De MNIST (\"Modified National Institute of Standards and Technology\")is een veelgebruikte dataset voor het testen en benchmarken van klassificatie algoritmes. Het bevat tienduizenden afbeeldingen van handgeschreven getallen. \n",
    "Meer info over deze dataset is te vinden op: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "\n",
    "Bij deze opdracht worden 3 databestanden meegeleverd:\n",
    "- 'train.csv' bevat gelabelde data voor **trainen** van  de classifier.\n",
    "- 'test.csv' bevat gelabelde data voor het **testen** van de getrainde classifier\n",
    "- 'test_Kaggle.csv' bevat niet-gelabelde data voor het evalueren van de classifier via de competitie te vinden op https://www.kaggle.com/c/digit-recognizer. Deze data is dus enkel bruikbaar bij deelname aan de Kaggle competitie. \n",
    "\n",
    "\n",
    "1) Train zowel een logistic regression classifier als een Support Vector Machine (met of zonder kernel. Ga via tuning van de hyperparameters op zoek naar de meest performante classifier. Doe dit door de accuracy te maximaliseren of de error rate te minimaliseren op de test.csv dataset. De error rate = 1- accuracy. Het trainen van Support Vector Machines (zeker deze met kernel) vragen enorm veel rekenkracht. Het is daarom verstandig om in eerste instantie te trainen op een klein deel van de training set. Het trainen via logistic regression is minder belastend voor de CPU, desalniettemin wordt aangeraden om de lbfgs solver te gebruiken. *(LogisticRegression(multi_class='multinomial', solver='lbfgs'))*\n",
    "\n",
    "2) Verzorg de code telkens van commentaar en schrijf jouw conclusies en besluiten neer.\n",
    "\n",
    "\n",
    "3) Is het nodig om hier te normaliseren? Welke normalisatie zou je gebruiken? Is de standardscaler een goede keuze?\n",
    "\n",
    "4) Onderzoek de twee verschillende types van multiclass classification: one-vs-one (ovo) of one-vs-rest (ovr). Kijk vooral naar accuracy en berekeningstijd. Wat zijn de conclusies? \n",
    "\n",
    "5) Test jouw uiteindelijke classifier met een aantal zelf geschreven getallen. Wat zijn de bevindingen? Waarvan hangt classificatienauwkeurigheid af?\n",
    "\n",
    "6) Optioneel: test op de 'test_Kaggle' dataset en laad de resultaten in het juiste formaat op naar de Kaggle website. Wat is de behaalde score? Vergelijk deze score met de score op http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "\n",
    "Het visualiseren van een digit kan met *'plt.imshow(X_train[n].reshape((28, 28)),cmap = 'gray')'*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0       1       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       1       0       0       0       0       0       0       0       0   \n",
       "3       4       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "..    ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "95      9       0       0       0       0       0       0       0       0   \n",
       "96      1       0       0       0       0       0       0       0       0   \n",
       "97      2       0       0       0       0       0       0       0       0   \n",
       "98      0       0       0       0       0       0       0       0       0   \n",
       "99      5       0       0       0       0       0       0       0       0   \n",
       "\n",
       "    pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0        0  ...         0         0         0         0         0         0   \n",
       "1        0  ...         0         0         0         0         0         0   \n",
       "2        0  ...         0         0         0         0         0         0   \n",
       "3        0  ...         0         0         0         0         0         0   \n",
       "4        0  ...         0         0         0         0         0         0   \n",
       "..     ...  ...       ...       ...       ...       ...       ...       ...   \n",
       "95       0  ...         0         0         0         0         0         0   \n",
       "96       0  ...         0         0         0         0         0         0   \n",
       "97       0  ...         0         0         0         0         0         0   \n",
       "98       0  ...         0         0         0         0         0         0   \n",
       "99       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "    pixel780  pixel781  pixel782  pixel783  \n",
       "0          0         0         0         0  \n",
       "1          0         0         0         0  \n",
       "2          0         0         0         0  \n",
       "3          0         0         0         0  \n",
       "4          0         0         0         0  \n",
       "..       ...       ...       ...       ...  \n",
       "95         0         0         0         0  \n",
       "96         0         0         0         0  \n",
       "97         0         0         0         0  \n",
       "98         0         0         0         0  \n",
       "99         0         0         0         0  \n",
       "\n",
       "[100 rows x 785 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inlezen van de dataset\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "df_train.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[3 2 2 ... 3 6 9]\n"
     ]
    }
   ],
   "source": [
    "# Opsplitsen in features en target\n",
    "X = df_train.drop(['label'], axis=1).values\n",
    "y = df_train['label'].values\n",
    "\n",
    "# Opsplitsen in training set en test set. - train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Opsplitsen in features en target\n",
    "X = df_test.drop(['label'], axis=1).values\n",
    "y = df_test['label'].values\n",
    "\n",
    "# Opsplitsen in training set en test set.  - test\n",
    "train_x, X_test, train_y, y_test = train_test_split(X, y, test_size=4500, random_state=0)\n",
    "\n",
    "#print('x test:', X_test.shape)\n",
    "#print('y test:', y_test.shape)\n",
    "\n",
    "#print('x train:', X_train.shape)\n",
    "#print('y train:', y_train.shape)\n",
    "\n",
    "# Normalisatie van de features is nodig\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialiseren en trainen van de logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainen van logistic regression via cross-validation\n",
    "\n",
    "# Initialiseren van de logistic regression classifier\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1, class_weight = None , multi_class='multinomial', solver='lbfgs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De bias = theta_0:  [-4.20834807 -4.48844875  2.56799816  3.09816911 -2.08326299  4.03747629\n",
      " -1.79047723 -2.41001208  5.12980659  0.14709897]\n",
      "De coefficienten theta:  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dominic Ho\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Training van het model\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print('De bias = theta_0: ', logreg.intercept_)\n",
    "print('De coefficienten theta: ',logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen van het model op de test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       436\n",
      "           1       0.94      0.97      0.95       509\n",
      "           2       0.84      0.84      0.84       434\n",
      "           3       0.88      0.86      0.87       465\n",
      "           4       0.86      0.92      0.89       412\n",
      "           5       0.84      0.84      0.84       395\n",
      "           6       0.91      0.94      0.92       464\n",
      "           7       0.89      0.90      0.90       461\n",
      "           8       0.86      0.79      0.83       428\n",
      "           9       0.88      0.84      0.86       496\n",
      "\n",
      "    accuracy                           0.88      4500\n",
      "   macro avg       0.88      0.88      0.88      4500\n",
      "weighted avg       0.88      0.88      0.88      4500\n",
      "\n",
      "[[406   0   4   2   0   3  14   1   5   1]\n",
      " [  0 496   3   0   1   2   1   1   3   2]\n",
      " [ 11   5 363  16   5   2   9   6  15   2]\n",
      " [  1   3  17 399   0  22   3   6   7   7]\n",
      " [  2   2   3   0 377   3   5   0   4  16]\n",
      " [  6   5   9   9  11 333   7   0  10   5]\n",
      " [  4   1   8   2   4   6 435   0   4   0]\n",
      " [  3   2   9   3  13   0   0 415   4  12]\n",
      " [  2  14  15  16   4  20   5   2 340  10]\n",
      " [  3   2   3   7  23   5   1  33   4 415]]\n",
      "88.42222222222222\n"
     ]
    }
   ],
   "source": [
    "# Testen van het model op de test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalueren van het model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  16 | elapsed:   24.5s remaining:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:   25.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9070666666666667\n",
      "Best parameters : {'C': 0.01, 'class_weight': 'balanced', 'multi_class': 'multinomial', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       436\n",
      "           1       0.94      0.99      0.96       509\n",
      "           2       0.90      0.88      0.89       434\n",
      "           3       0.93      0.88      0.91       465\n",
      "           4       0.90      0.92      0.91       412\n",
      "           5       0.88      0.86      0.87       395\n",
      "           6       0.94      0.96      0.95       464\n",
      "           7       0.92      0.93      0.93       461\n",
      "           8       0.88      0.86      0.87       428\n",
      "           9       0.90      0.88      0.89       496\n",
      "\n",
      "    accuracy                           0.91      4500\n",
      "   macro avg       0.91      0.91      0.91      4500\n",
      "weighted avg       0.91      0.91      0.91      4500\n",
      "\n",
      "[[420   0   2   0   0   3   8   0   3   0]\n",
      " [  0 503   0   0   1   1   0   1   3   0]\n",
      " [  5   1 383   9   5   4   7   5  15   0]\n",
      " [  1   4  15 409   0  15   1   4   8   8]\n",
      " [  1   2   4   0 381   2   2   0   4  16]\n",
      " [  6   7   5   6   8 341   8   1   8   5]\n",
      " [  3   1   3   0   3   4 446   0   4   0]\n",
      " [  2   1   8   3   5   0   0 430   1  11]\n",
      " [  2  18   4   7   1  16   4   2 367   7]\n",
      " [  5   0   2   4  21   2   0  25   2 435]]\n",
      "91.44444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dominic Ho\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create instance logistic regression\n",
    "model = LogisticRegression()\n",
    "# assign parameters for grid search\n",
    "paramaters = [\n",
    "             {'C': [0.01, 0.01, 1, 10],  \n",
    "              'solver': ['lbfgs'], \n",
    "              'class_weight': [None, 'balanced'],\n",
    "              'multi_class': ['multinomial']},\n",
    "             ]\n",
    "\n",
    "# create variable grid search with the required parameters\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 2,\n",
    "                           n_jobs = -1,\n",
    "                           verbose=5)\n",
    "\n",
    "# train grid search on the training data\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# assign the best score and best parameters to variables\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "# predict on grid search with test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   15.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.8721333333333333\n",
      "Best parameters : {'C': 5.217087859902357, 'class_weight': 'balanced', 'multi_class': 'multinomial', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       436\n",
      "           1       0.93      0.96      0.95       509\n",
      "           2       0.82      0.83      0.82       434\n",
      "           3       0.86      0.85      0.86       465\n",
      "           4       0.85      0.90      0.87       412\n",
      "           5       0.82      0.84      0.83       395\n",
      "           6       0.92      0.94      0.93       464\n",
      "           7       0.88      0.89      0.89       461\n",
      "           8       0.85      0.77      0.81       428\n",
      "           9       0.86      0.83      0.84       496\n",
      "\n",
      "    accuracy                           0.87      4500\n",
      "   macro avg       0.87      0.87      0.87      4500\n",
      "weighted avg       0.87      0.87      0.87      4500\n",
      "\n",
      "[[402   0   5   2   0   7  13   1   5   1]\n",
      " [  1 489   4   1   3   3   1   1   4   2]\n",
      " [ 11   5 359  17   5   4   7   7  17   2]\n",
      " [  1   4  19 397   0  21   2   7   8   6]\n",
      " [  2   3   6   0 369   4   5   0   1  22]\n",
      " [  5   5   8  12  11 330   7   0  12   5]\n",
      " [  4   1   8   1   4   6 436   0   4   0]\n",
      " [  3   2   9   3  14   0   0 410   4  16]\n",
      " [  3  14  17  19   4  23   4   2 329  13]\n",
      " [  3   2   5   7  23   6   0  37   3 410]]\n",
      "87.35555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dominic Ho\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train een SVM via random search\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint \n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create instance logistic regression\n",
    "model = LogisticRegression()\n",
    "\n",
    "# assign parameters for random search\n",
    "parameters = [\n",
    "             {'C' : uniform(0.01, 20), \n",
    "              'solver': ['lbfgs'],\n",
    "              'class_weight': [None, 'balanced'],\n",
    "              'multi_class': ['multinomial']}\n",
    "             ]\n",
    "\n",
    "              \n",
    "# amount ranndom search\n",
    "n_iter_search = 5\n",
    "\n",
    "# create variable random search with the required parameters\n",
    "random_search = RandomizedSearchCV(model, param_distributions=parameters,cv=2,n_iter=n_iter_search,n_jobs = -1,verbose=1)\n",
    "\n",
    "# train random search on the training data\n",
    "random_search = random_search.fit(X_train, y_train)\n",
    "\n",
    "# assign the best score and best parameters to variables\n",
    "best_accuracy = random_search.best_score_ \n",
    "best_parameters = random_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', random_search.best_score_)\n",
    "print('Best parameters :',random_search.best_params_  )\n",
    "\n",
    "# predict on random search with test set\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "    - Logistic regression on test set:\n",
    "        - Accuracy: 88.42%\n",
    "        \n",
    "    - Grid search:\n",
    "        - Best accuracy:  0.8675333333333333 %\n",
    "        - Accuracy:91.44%\n",
    "    - Random search:\n",
    "        - Best accuracy: 0.8657333333333332 %\n",
    "        - Accuracy : 87.35%\n",
    "\n",
    "\n",
    "The results of the logistic regression amounts 89.77% for the accuracy and the preference goes to the grid search method for improving the model as the accuracy amounts 91.44% which has improved.\n",
    "\n",
    "In general the f1 scores from the classifcation report is in the range between 0.81 - 0.95 between the 10 classes. As the lowest is 0.81, there is room for improvement so the model can be more efficient and reliable for making predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialiseren en trainen van SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, kernel='poly')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainen van SVM via cross-validation\n",
    "\n",
    "# Initialiseren van de SVM classifier\n",
    "SVMkernel = svm.SVC(kernel='poly',C=1)\n",
    "SVMkernel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen van het model op de test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       436\n",
      "           1       0.98      0.99      0.98       509\n",
      "           2       0.95      0.88      0.92       434\n",
      "           3       0.97      0.91      0.94       465\n",
      "           4       0.86      0.96      0.91       412\n",
      "           5       0.95      0.90      0.93       395\n",
      "           6       0.97      0.93      0.95       464\n",
      "           7       0.97      0.90      0.93       461\n",
      "           8       0.77      0.96      0.85       428\n",
      "           9       0.93      0.91      0.92       496\n",
      "\n",
      "    accuracy                           0.93      4500\n",
      "   macro avg       0.93      0.93      0.93      4500\n",
      "weighted avg       0.93      0.93      0.93      4500\n",
      "\n",
      "[[416   0   1   0   0   0   9   0  10   0]\n",
      " [  1 503   0   0   1   0   0   0   4   0]\n",
      " [  4   0 383   3   5   0   1   1  37   0]\n",
      " [  0   2   6 421   1   4   0   1  27   3]\n",
      " [  1   2   3   1 394   2   1   0   2   6]\n",
      " [  1   0   2   4   4 357   2   0  21   4]\n",
      " [  2   1   1   1   6   7 431   0  15   0]\n",
      " [  1   2   3   1  20   1   0 414   4  15]\n",
      " [  1   3   1   2   2   4   0   0 410   5]\n",
      " [  3   0   2   1  24   0   0  11   5 450]]\n",
      "92.86666666666666\n"
     ]
    }
   ],
   "source": [
    "# Training van het model\n",
    "\n",
    "y_pred = SVMkernel.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalueren van het model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 108 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 66.3min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 146.6min\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed: 183.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9507333333333334\n",
      "Best parameters : {'C': 0.01, 'class_weight': None, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'kernel': 'poly'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       436\n",
      "           1       0.99      1.00      0.99       509\n",
      "           2       0.95      0.96      0.96       434\n",
      "           3       0.95      0.97      0.96       465\n",
      "           4       0.94      0.97      0.95       412\n",
      "           5       0.97      0.94      0.96       395\n",
      "           6       0.98      0.96      0.97       464\n",
      "           7       0.96      0.96      0.96       461\n",
      "           8       0.94      0.95      0.95       428\n",
      "           9       0.96      0.93      0.95       496\n",
      "\n",
      "    accuracy                           0.96      4500\n",
      "   macro avg       0.96      0.96      0.96      4500\n",
      "weighted avg       0.96      0.96      0.96      4500\n",
      "\n",
      "[[425   0   2   0   0   1   7   0   1   0]\n",
      " [  1 507   0   0   0   0   0   0   1   0]\n",
      " [  5   0 416   4   1   1   1   1   5   0]\n",
      " [  0   1   5 449   0   2   0   1   5   2]\n",
      " [  1   1   2   1 398   2   0   0   1   6]\n",
      " [  1   0   1  10   4 373   2   0   2   2]\n",
      " [  3   0   3   1   3   3 447   0   4   0]\n",
      " [  1   1   4   1   6   0   0 441   0   7]\n",
      " [  2   3   3   5   2   4   0   1 406   2]\n",
      " [  2   0   0   1   9   0   0  16   5 463]]\n",
      "96.11111111111111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.learning_curve import learning_curve\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn import cross_validation\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "\n",
    "# create instance SVC\n",
    "model = SVC()\n",
    "\n",
    "# assign parameters for random search\n",
    "paramaters = [ \n",
    "        {'kernel': ['rbf', 'poly', 'sigmoid'], \n",
    "         'C': [0.01, 1, 10], \n",
    "         'gamma': [0.001, 0.01, 0.1], \n",
    "         'class_weight': [None, 'balanced'], \n",
    "         'decision_function_shape': ['ovo', 'ovr']},\n",
    "        ]\n",
    "\n",
    "# create variable grid search with the required parameters\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 2, \n",
    "                           n_jobs = -1,\n",
    "                           verbose = 5)\n",
    "\n",
    "# train grid search on the training data\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# assign the best score and best parameters to variables\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "# predict on grid search with test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  8.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9507333333333334\n",
      "Best parameters : {'C': 18.453429347029257, 'class_weight': None, 'decision_function_shape': 'ovr', 'gamma': 0.17460634715043422, 'kernel': 'poly'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       436\n",
      "           1       0.99      1.00      0.99       509\n",
      "           2       0.95      0.96      0.96       434\n",
      "           3       0.95      0.97      0.96       465\n",
      "           4       0.94      0.97      0.95       412\n",
      "           5       0.97      0.94      0.96       395\n",
      "           6       0.98      0.96      0.97       464\n",
      "           7       0.96      0.96      0.96       461\n",
      "           8       0.94      0.95      0.95       428\n",
      "           9       0.96      0.93      0.95       496\n",
      "\n",
      "    accuracy                           0.96      4500\n",
      "   macro avg       0.96      0.96      0.96      4500\n",
      "weighted avg       0.96      0.96      0.96      4500\n",
      "\n",
      "[[425   0   2   0   0   1   7   0   1   0]\n",
      " [  1 507   0   0   0   0   0   0   1   0]\n",
      " [  5   0 416   4   1   1   1   1   5   0]\n",
      " [  0   1   5 449   0   2   0   1   5   2]\n",
      " [  1   1   2   1 398   2   0   0   1   6]\n",
      " [  1   0   1  10   4 373   2   0   2   2]\n",
      " [  3   0   3   1   3   3 447   0   4   0]\n",
      " [  1   1   4   1   6   0   0 441   0   7]\n",
      " [  2   3   3   5   2   4   0   1 406   2]\n",
      " [  2   0   0   1   9   0   0  16   5 463]]\n",
      "96.11111111111111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint \n",
    "from scipy.stats import uniform\n",
    "\n",
    "\n",
    "# create instance SVC\n",
    "model = SVC()\n",
    "\n",
    "# assign parameters for random search\n",
    "parameters = [\n",
    "    {'kernel': ['rbf', 'poly', 'sigmoid'], \n",
    "     'C': uniform(0.01, 20), \n",
    "     'gamma': uniform(0.001, 0.2), \n",
    "     'class_weight': [None, 'balanced'], \n",
    "     'decision_function_shape': ['ovo', 'ovr']},\n",
    "    ]\n",
    "\n",
    "\n",
    "# amount random search\n",
    "n_iter_search = 5\n",
    "\n",
    "# create variable random search with the required parameters\n",
    "random_search = RandomizedSearchCV(model, param_distributions=parameters,cv=2,n_iter=n_iter_search,n_jobs = -1,verbose=1)\n",
    "\n",
    "# train random search on the training data\n",
    "random_search = random_search.fit(X_train, y_train)\n",
    "\n",
    "# assign the best score and best parameters to variables\n",
    "best_accuracy = random_search.best_score_ \n",
    "best_parameters = random_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', random_search.best_score_)\n",
    "print('Best parameters :',random_search.best_params_  )\n",
    "\n",
    "# predict on random search with test set\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "    -SVM on test set:\n",
    "        - Accuracy: 92.86%\n",
    "    - Grid search:\n",
    "        - Best accuracy: 0.9507 %\n",
    "        - Accuracy: 96.11%\n",
    "    - Random search:\n",
    "        - Best accuracy: 0.9507 %\n",
    "        - Accuracy 96.11%\n",
    "\n",
    "The results of the logistic regression amounts 92.86% for the accuracy and the preference goes to the grid and random search method for improving the model as the accuracy are both similar with 96.11%. The model has improved.\n",
    "\n",
    "In general the f1 scores from the classifcation report is in the range between 0.95 - 0.99 between the 10 classes. As the lowest This is an ideal model for making predictions.\n",
    "\n",
    "Conclusion: Based on the results we can say the the SVM classifier with grid search or random search method gets the best model with an accuracy of 96.11%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Is het nodig om hier te normaliseren? Welke normalisatie zou je gebruiken? Is de standardscaler een goede keuze?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is not required but always recommended as the feature values will be scaled and that it will speed up the execution time. Standardisation is a technique that pretty much works all the time compared with the other techniques. Therefore, it is recommended to use standardscaler as it always will do some relevant feature scaling and improve the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Onderzoek de twee verschillende types van multiclass classification: one-vs-one (ovo) of one-vs-rest (ovr). Kijk vooral naar accuracy en berekeningstijd. Wat zijn de conclusies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the 2 different types of multiclass classification is the one-vs-rest (over) the assigned parameter for both grid and random search. In this case, the ovr is the best parameter which means that it certainly improves the accuracy rate and execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Test jouw uiteindelijke classifier met een aantal zelf geschreven getallen. Wat zijn de bevindingen? Waarvan hangt classificatienauwkeurigheid af?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opdracht 3 - 50K a year\n",
    "\n",
    "Train een classifier die zo accuraat mogelijk kan voorspellen of iemand al dan niet dan 50000 dollar per jaar verdient. Gebruik hiervoor de dataset '50K_a_year.csv'.\n",
    "\n",
    "Train zowel een logistic regression classifier als een Support Vector Machine (met of zonder kernel. Ga via tuning van de hyperparameters op zoek naar de meest performante classifier. Bespreek en argumentaar welke classifier jouw voorkeur geniet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   workclass  fnlwgt      education  education-num       marital-status  \\\n",
       "0   25     Private  226802           11th              7        Never-married   \n",
       "1   38     Private   89814        HS-grad              9   Married-civ-spouse   \n",
       "2   28   Local-gov  336951     Assoc-acdm             12   Married-civ-spouse   \n",
       "3   44     Private  160323   Some-college             10   Married-civ-spouse   \n",
       "4   18           ?  103497   Some-college             10        Never-married   \n",
       "\n",
       "           occupation relationship    race      sex  capital-gain  \\\n",
       "0   Machine-op-inspct    Own-child   Black     Male             0   \n",
       "1     Farming-fishing      Husband   White     Male             0   \n",
       "2     Protective-serv      Husband   White     Male             0   \n",
       "3   Machine-op-inspct      Husband   Black     Male          7688   \n",
       "4                   ?    Own-child   White   Female             0   \n",
       "\n",
       "   capital-loss  hours-per-week  native-country   class  \n",
       "0             0              40   United-States   <=50K  \n",
       "1             0              50   United-States   <=50K  \n",
       "2             0              40   United-States    >50K  \n",
       "3             0              40   United-States    >50K  \n",
       "4             0              30   United-States   <=50K  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('50K_a_year.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education-num     0\n",
       "marital-status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital-gain      0\n",
       "capital-loss      0\n",
       "hours-per-week    0\n",
       "native-country    0\n",
       "class             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inconsistenties opsporen\n",
    "dataset.isnull().sum()\n",
    "# dataset.isna().any().any()\n",
    "# neen, er zijn geen ontbrekende waarden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48842.000000</td>\n",
       "      <td>4.884200e+04</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.643585</td>\n",
       "      <td>1.896641e+05</td>\n",
       "      <td>10.078089</td>\n",
       "      <td>1079.067626</td>\n",
       "      <td>87.502314</td>\n",
       "      <td>40.422382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.710510</td>\n",
       "      <td>1.056040e+05</td>\n",
       "      <td>2.570973</td>\n",
       "      <td>7452.019058</td>\n",
       "      <td>403.004552</td>\n",
       "      <td>12.391444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.175505e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.781445e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.376420e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.490400e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "count  48842.000000  4.884200e+04   48842.000000  48842.000000  48842.000000   \n",
       "mean      38.643585  1.896641e+05      10.078089   1079.067626     87.502314   \n",
       "std       13.710510  1.056040e+05       2.570973   7452.019058    403.004552   \n",
       "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.175505e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.781445e+05      10.000000      0.000000      0.000000   \n",
       "75%       48.000000  2.376420e+05      12.000000      0.000000      0.000000   \n",
       "max       90.000000  1.490400e+06      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       hours-per-week  \n",
       "count    48842.000000  \n",
       "mean        40.422382  \n",
       "std         12.391444  \n",
       "min          1.000000  \n",
       "25%         40.000000  \n",
       "50%         40.000000  \n",
       "75%         45.000000  \n",
       "max         99.000000  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             48842 non-null  int64 \n",
      " 1   workclass       48842 non-null  object\n",
      " 2   fnlwgt          48842 non-null  int64 \n",
      " 3   education       48842 non-null  object\n",
      " 4   education-num   48842 non-null  int64 \n",
      " 5   marital-status  48842 non-null  object\n",
      " 6   occupation      48842 non-null  object\n",
      " 7   relationship    48842 non-null  object\n",
      " 8   race            48842 non-null  object\n",
      " 9   sex             48842 non-null  object\n",
      " 10  capital-gain    48842 non-null  int64 \n",
      " 11  capital-loss    48842 non-null  int64 \n",
      " 12  hours-per-week  48842 non-null  int64 \n",
      " 13  native-country  48842 non-null  object\n",
      " 14  class           48842 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>class</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>workclass_ Never-worked</th>\n",
       "      <th>workclass_ Private</th>\n",
       "      <th>workclass_ Self-emp-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>country_ Scotland</th>\n",
       "      <th>country_ South</th>\n",
       "      <th>country_ Taiwan</th>\n",
       "      <th>country_ Thailand</th>\n",
       "      <th>country_ Trinadad&amp;Tobago</th>\n",
       "      <th>country_ United-States</th>\n",
       "      <th>country_ Vietnam</th>\n",
       "      <th>country_ Yugoslavia</th>\n",
       "      <th>sex_ Female</th>\n",
       "      <th>sex_ Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>226802</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>89814</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>336951</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>160323</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>103497</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>27</td>\n",
       "      <td>257302</td>\n",
       "      <td>12</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>40</td>\n",
       "      <td>154374</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>58</td>\n",
       "      <td>151910</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>22</td>\n",
       "      <td>201490</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>52</td>\n",
       "      <td>287927</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fnlwgt  education-num  hours-per-week  class  \\\n",
       "0       25  226802              7              40      0   \n",
       "1       38   89814              9              50      0   \n",
       "2       28  336951             12              40      1   \n",
       "3       44  160323             10              40      1   \n",
       "4       18  103497             10              30      0   \n",
       "...    ...     ...            ...             ...    ...   \n",
       "48837   27  257302             12              38      0   \n",
       "48838   40  154374              9              40      1   \n",
       "48839   58  151910              9              40      0   \n",
       "48840   22  201490              9              20      0   \n",
       "48841   52  287927              9              40      1   \n",
       "\n",
       "       workclass_ Federal-gov  workclass_ Local-gov  workclass_ Never-worked  \\\n",
       "0                           0                     0                        0   \n",
       "1                           0                     0                        0   \n",
       "2                           0                     1                        0   \n",
       "3                           0                     0                        0   \n",
       "4                           0                     0                        0   \n",
       "...                       ...                   ...                      ...   \n",
       "48837                       0                     0                        0   \n",
       "48838                       0                     0                        0   \n",
       "48839                       0                     0                        0   \n",
       "48840                       0                     0                        0   \n",
       "48841                       0                     0                        0   \n",
       "\n",
       "       workclass_ Private  workclass_ Self-emp-inc  ...  country_ Scotland  \\\n",
       "0                       1                        0  ...                  0   \n",
       "1                       1                        0  ...                  0   \n",
       "2                       0                        0  ...                  0   \n",
       "3                       1                        0  ...                  0   \n",
       "4                       0                        0  ...                  0   \n",
       "...                   ...                      ...  ...                ...   \n",
       "48837                   1                        0  ...                  0   \n",
       "48838                   1                        0  ...                  0   \n",
       "48839                   1                        0  ...                  0   \n",
       "48840                   1                        0  ...                  0   \n",
       "48841                   0                        1  ...                  0   \n",
       "\n",
       "       country_ South  country_ Taiwan  country_ Thailand  \\\n",
       "0                   0                0                  0   \n",
       "1                   0                0                  0   \n",
       "2                   0                0                  0   \n",
       "3                   0                0                  0   \n",
       "4                   0                0                  0   \n",
       "...               ...              ...                ...   \n",
       "48837               0                0                  0   \n",
       "48838               0                0                  0   \n",
       "48839               0                0                  0   \n",
       "48840               0                0                  0   \n",
       "48841               0                0                  0   \n",
       "\n",
       "       country_ Trinadad&Tobago  country_ United-States  country_ Vietnam  \\\n",
       "0                             0                       1                 0   \n",
       "1                             0                       1                 0   \n",
       "2                             0                       1                 0   \n",
       "3                             0                       1                 0   \n",
       "4                             0                       1                 0   \n",
       "...                         ...                     ...               ...   \n",
       "48837                         0                       1                 0   \n",
       "48838                         0                       1                 0   \n",
       "48839                         0                       1                 0   \n",
       "48840                         0                       1                 0   \n",
       "48841                         0                       1                 0   \n",
       "\n",
       "       country_ Yugoslavia  sex_ Female  sex_ Male  \n",
       "0                        0            0          1  \n",
       "1                        0            0          1  \n",
       "2                        0            0          1  \n",
       "3                        0            0          1  \n",
       "4                        0            1          0  \n",
       "...                    ...          ...        ...  \n",
       "48837                    0            1          0  \n",
       "48838                    0            0          1  \n",
       "48839                    0            1          0  \n",
       "48840                    0            0          1  \n",
       "48841                    0            1          0  \n",
       "\n",
       "[48842 rows x 88 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove columns\n",
    "\n",
    "dataset.drop(['capital-gain'],axis=1, inplace=True)\n",
    "dataset.drop(['capital-loss'],axis=1, inplace=True)\n",
    "dataset.drop(['education'], axis=1, inplace=True)\n",
    "\n",
    "# one hot encoding\n",
    "dataset = pd.concat([dataset,pd.get_dummies(dataset['workclass'], prefix='workclass')],axis=1)\n",
    "dataset.drop(['workclass'],axis=1, inplace=True)\n",
    "\n",
    "dataset = pd.concat([dataset,pd.get_dummies(dataset['marital-status'], prefix='marital-status')],axis=1)\n",
    "dataset.drop(['marital-status'],axis=1, inplace=True)\n",
    "\n",
    "dataset = pd.concat([dataset,pd.get_dummies(dataset['occupation'], prefix='occupation')],axis=1)\n",
    "dataset.drop(['occupation'],axis=1, inplace=True)\n",
    "\n",
    "dataset = pd.concat([dataset,pd.get_dummies(dataset['relationship'], prefix='relationship')],axis=1)\n",
    "dataset.drop(['relationship'],axis=1, inplace=True)\n",
    "\n",
    "dataset = pd.concat([dataset,pd.get_dummies(dataset['race'], prefix='race')],axis=1)\n",
    "dataset.drop(['race'],axis=1, inplace=True)\n",
    "\n",
    "dataset = pd.concat([dataset,pd.get_dummies(dataset['native-country'], prefix='country')],axis=1)\n",
    "dataset.drop(['native-country'],axis=1, inplace=True)\n",
    "\n",
    "dataset = pd.concat([dataset,pd.get_dummies(dataset['sex'], prefix='sex')],axis=1)\n",
    "dataset.drop(['sex'],axis=1, inplace=True)\n",
    "\n",
    "dataset['class'] = dataset['class'].map({' >50K': 1, ' <=50K': 0})\n",
    "\n",
    "#print(dataset.columns.tolist())\n",
    "# workclass_ ?\n",
    "dataset.drop(['workclass_ ?'], axis=1, inplace=True)\n",
    "# occupation_ ?\n",
    "dataset.drop(['occupation_ ?'], axis=1, inplace=True)\n",
    "# country_ ?\n",
    "dataset.drop(['country_ ?'], axis=1, inplace=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (39073, 87)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Opsplitsen in features en targets\n",
    "X = dataset.drop(['class'], axis=1).values\n",
    "y = dataset['class'].values\n",
    "\n",
    "# Opsplitsen in training set en test set. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Shape:',X_train.shape)\n",
    "\n",
    "# Normalisatie van de features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[:,:4] = scaler.fit_transform(X_train[:,:4])\n",
    "X_test[:,:4] = scaler.transform(X_test[:,:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialiseren en trainen van de logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainen van logistic regression via cross-validation\n",
    "\n",
    "# Initialiseren van de logistic regression classifier\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1, class_weight = None, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De bias = theta_0:  [-2.39415616]\n",
      "De coefficienten theta:  [[ 0.23748732  0.11492111  0.99590255  0.32734201  0.95113027  0.22618169\n",
      "  -0.14353214  0.33337906  0.7755611  -0.06777578 -0.06044241 -0.48151403\n",
      "  -0.55524405  1.11996837  1.31427119 -0.58561712 -1.26957863 -0.73557227\n",
      "  -0.40822588  0.24117175  0.22876158  0.2595456   1.15127216 -0.56579132\n",
      "  -0.53230983 -0.12558763 -0.68826009 -1.45725952  1.00606893  0.62507425\n",
      "   0.57097198  0.85416471  0.10869734 -0.15621473  0.17150855 -0.79499975\n",
      "  -1.10945719 -0.08544804  0.85461277 -0.62196867  0.05624469 -0.34883068\n",
      "  -0.13019417 -0.07524956  0.52983656  0.67120149 -0.42351274 -1.56937658\n",
      "   0.11657618 -0.89515304 -0.36062697 -0.7289751   0.66776965  0.55053382\n",
      "   0.15911992  0.00686893 -0.30890796 -0.12018055 -0.00302039 -0.00745274\n",
      "  -0.68283427  0.25321276  0.04006241  0.17492427  0.5083663   0.73958392\n",
      "   0.34617966  0.20253042 -0.71280657 -0.49440196 -0.5740015  -0.65268415\n",
      "  -0.97023628  0.38759792 -0.04798908  0.38965138 -0.18984865 -0.50053648\n",
      "  -1.02454039 -0.25193894 -0.7067299  -1.09176652  0.24796044 -0.65083098\n",
      "   1.10477921 -0.95502027 -0.16497813]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dominic Ho\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Training van het model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('De bias = theta_0: ', logreg.intercept_)\n",
    "print('De coefficienten theta: ',logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen van het model op de test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89      7420\n",
      "           1       0.68      0.53      0.60      2349\n",
      "\n",
      "    accuracy                           0.83      9769\n",
      "   macro avg       0.77      0.73      0.74      9769\n",
      "weighted avg       0.82      0.83      0.82      9769\n",
      "\n",
      "[[6830  590]\n",
      " [1103 1246]]\n",
      "82.66966936226841\n"
     ]
    }
   ],
   "source": [
    "# Testen van het model op de test set\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalueren van het model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 60 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.833439930616122\n",
      "Best parameters : {'C': 1, 'class_weight': None, 'multi_class': 'ovr', 'solver': 'saga'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89      7420\n",
      "           1       0.68      0.53      0.59      2349\n",
      "\n",
      "    accuracy                           0.83      9769\n",
      "   macro avg       0.77      0.72      0.74      9769\n",
      "weighted avg       0.82      0.83      0.82      9769\n",
      "\n",
      "[[6833  587]\n",
      " [1110 1239]]\n",
      "82.62872351315386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dominic Ho\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create instance logistic regression\n",
    "model = LogisticRegression()\n",
    "# assign parameters for grid search\n",
    "paramaters = [\n",
    "             {'C' : [0.01, 1, 10], \n",
    "              'solver': ['liblinear','lbfgs', 'newton-cg', 'sag','saga'], \n",
    "              'class_weight': [None, 'balanced'],\n",
    "              'multi_class': ['multinomial', 'ovr'],\n",
    "             }\n",
    "             ]\n",
    "\n",
    "# create variable grid search with the required parameters\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 2,\n",
    "                           n_jobs = -1,\n",
    "                           verbose=5)\n",
    "\n",
    "# train grid search on the training data\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# assign the best score and best parameters to variables\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "# predict on grid search with test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   29.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.8327743844774893\n",
      "Best parameters : {'C': 8.568324096769386, 'class_weight': None, 'multi_class': 'ovr', 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89      7420\n",
      "           1       0.68      0.53      0.59      2349\n",
      "\n",
      "    accuracy                           0.83      9769\n",
      "   macro avg       0.77      0.72      0.74      9769\n",
      "weighted avg       0.82      0.83      0.82      9769\n",
      "\n",
      "[[6834  586]\n",
      " [1108 1241]]\n",
      "82.65943289998975\n"
     ]
    }
   ],
   "source": [
    "# Train een SVM via random search\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint \n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create instance logistic regression\n",
    "model = LogisticRegression()\n",
    "\n",
    "# assign parameters for random search\n",
    "parameters = [\n",
    "             {'C' : uniform(0.01, 20), \n",
    "              'solver': ['liblinear','lbfgs', 'newton-cg', 'sag','saga'],\n",
    "              'class_weight': [None, 'balanced'],\n",
    "              'multi_class': ['multinomial', 'ovr']},\n",
    "             ]\n",
    "\n",
    "              \n",
    "# amount ranndom search\n",
    "n_iter_search = 5\n",
    "\n",
    "# create variable random search with the required parameters\n",
    "random_search = RandomizedSearchCV(model, param_distributions=parameters,cv=5,n_iter=n_iter_search,n_jobs = -1,verbose=1)\n",
    "\n",
    "# train random search on the training data\n",
    "random_search = random_search.fit(X_train, y_train)\n",
    "\n",
    "# assign the best score and best parameters to variables\n",
    "best_accuracy = random_search.best_score_ \n",
    "best_parameters = random_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', random_search.best_score_)\n",
    "print('Best parameters :',random_search.best_params_  )\n",
    "\n",
    "# predict on random search with test set\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the logistic regression amounts 82.66% for the accuracy and the preference goes to the grid search method for improving the model as the best accuracy amounts 0.8334% which has improved slightly with a cv value of 2. \n",
    "\n",
    "From our confusion matrix, we can see that our model got (6833+1239) 8072 predictions right and (1110+587) 1697 predictions wrong. We have 587 false positive which means that our model predicted that 587 person were earning more than 50k but it turned out not to be. On the other hand, our model also has 1110 false negative. What this means is that our model predicted 1110 people earning less than 50k while they are positive. \n",
    "\n",
    "Moreover based on the classification report there is a preference for class 0 with 0.89% compared with class 1 with 0.59%. This model is ok but can certainly be improved to be in the range of 0.90-1.00% which makes the model ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialiseren en trainen van SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, kernel='linear')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainen van SVM via cross-validation\n",
    "SVMlinear = svm.SVC(kernel='linear',C=1)\n",
    "SVMlinear.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen van het model op de test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89      7420\n",
      "           1       0.68      0.53      0.60      2349\n",
      "\n",
      "    accuracy                           0.83      9769\n",
      "   macro avg       0.77      0.73      0.74      9769\n",
      "weighted avg       0.82      0.83      0.82      9769\n",
      "\n",
      "[[6830  590]\n",
      " [1103 1246]]\n",
      "82.66966936226841\n"
     ]
    }
   ],
   "source": [
    "# Testen van het model op de test set\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalueren van het model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 360 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 25.4min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 59.7min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 131.1min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 197.7min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 308.7min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 363.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.8353594310352641\n",
      "Best parameters : {'C': 1, 'class_weight': None, 'decision_function_shape': 'ovo', 'degree': 2, 'gamma': 0.2, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89      7420\n",
      "           1       0.70      0.53      0.60      2349\n",
      "\n",
      "    accuracy                           0.83      9769\n",
      "   macro avg       0.78      0.73      0.75      9769\n",
      "weighted avg       0.82      0.83      0.82      9769\n",
      "\n",
      "[[6877  543]\n",
      " [1098 1251]]\n",
      "83.20196540075749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# create instance SVC\n",
    "model = SVC()\n",
    "\n",
    "# assign parameters for random search\n",
    "paramaters = [ \n",
    "        {'kernel': ['linear'], \n",
    "         'C': [0.01, 1, 10],\n",
    "         'degree': [2,3,4],\n",
    "         'class_weight': [None, 'balanced'],\n",
    "         'decision_function_shape': ['ovo', 'ovr'],\n",
    "        },\n",
    "        {'kernel': ['rbf', 'poly', 'sigmoid'], \n",
    "         'C': [0.01, 1, 10], \n",
    "         'gamma': [0.01, 0.1, 0.2],\n",
    "         'degree': [2,3,4],\n",
    "         'class_weight': [None, 'balanced'], \n",
    "         'decision_function_shape': ['ovo', 'ovr'],\n",
    "        },\n",
    "        ]\n",
    "\n",
    "# create variable grid search with the required parameters\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 2,\n",
    "                           n_jobs = -1,\n",
    "                           verbose =5)\n",
    "\n",
    "# train grid search on the training data\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# assign the best score and best parameters to variables\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "# predict on grid search with test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed: 40.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.8212575325780571\n",
      "Best parameters : {'C': 17.15682103644547, 'class_weight': None, 'decision_function_shape': 'ovr', 'degree': 2, 'kernel': 'linear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88      7420\n",
      "           1       0.63      0.54      0.58      2349\n",
      "\n",
      "    accuracy                           0.81      9769\n",
      "   macro avg       0.75      0.72      0.73      9769\n",
      "weighted avg       0.81      0.81      0.81      9769\n",
      "\n",
      "[[6687  733]\n",
      " [1076 1273]]\n",
      "81.48223973794656\n"
     ]
    }
   ],
   "source": [
    "# Train een SVM via random search\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# distributions\n",
    "from scipy.stats import randint \n",
    "from scipy.stats import uniform\n",
    "\n",
    "\n",
    "# create instance SVC\n",
    "model = SVC()\n",
    "\n",
    "# assign parameters for random search\n",
    "parameters = [\n",
    "    {'kernel': ['linear'], \n",
    "     'C': uniform(0.01, 20),\n",
    "     'degree': [2,3,4],\n",
    "     'class_weight': [None, 'balanced'], \n",
    "     'decision_function_shape': ['ovo', 'ovr'],\n",
    "    },\n",
    "    {'kernel': ['rbf', 'poly', 'sigmoid'], \n",
    "     'C': uniform(0.01, 20),\n",
    "     'degree': [2,3,4],\n",
    "     'gamma': uniform(0.001, 0.2), \n",
    "     'class_weight': [None, 'balanced'], \n",
    "     'decision_function_shape': ['ovo', 'ovr'],\n",
    "    }\n",
    "    ]\n",
    "\n",
    "# amount ranndom search\n",
    "n_iter_search = 5\n",
    "\n",
    "# create variable random search with the required parameters\n",
    "random_search = RandomizedSearchCV(model, param_distributions=parameters,cv=5,n_iter=n_iter_search,n_jobs = -1,verbose=1)\n",
    "\n",
    "# train random search on the training data\n",
    "random_search = random_search.fit(X_train, y_train)\n",
    "\n",
    "# assign the best score and best parameters to variables\n",
    "best_accuracy = random_search.best_score_ \n",
    "best_parameters = random_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', random_search.best_score_)\n",
    "print('Best parameters :',random_search.best_params_  )\n",
    "\n",
    "# predict on random search with test set\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the SVM amounts 82.66% for the accuracy and the preference goes to the grid search method for improving the model as the best accuracy amounts 83.20% which has improved slightly.\n",
    "\n",
    "From our confusion matrix, we can see that our model got (6877+1251) 8128 predictions right and (1098+543) 1641 predictions wrong. We have 543 false positive which means that our model predicted that 543 person were earning more than 50k but it turned out not to be. On the other hand, our model also has 1098 false negative. What this means is that our model predicted 1098 people earning less than 50k while they are positive. \n",
    "\n",
    "Moreover based on the classification report there is a preference for class 0 with 0.89% compared with class 1 with 0.60%. This model is ok but can certainly be improved to be in the range of 0.90-1.00% which makes the model ideal.\n",
    "\n",
    "Conclusion: Based on the results, the SVM classifier with the grid search method is the best and most efficient model with an accuracy of 83.20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
